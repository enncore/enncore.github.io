
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Security for all in an AI-enabled society">
    <meta name="author" content="EnnCore Team">
    <title>Security for all in an AI-enabled society</title>
    
    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web" rel="stylesheet">
    
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="bootstrap/js/popper.min.js"></script>

    <style>
        .custom-popover {
            max-width: 400px;
        }
        .custom-popover .popover-header {
            background-color:rgb(222, 240, 255);
        }
    </style>
</head>

<body id="secaiws">
    <header id="header" class="container-fluid"></header>

    <!-- Page Content -->
    <div class="container">
        <div class="row mb-2">
            <div class="col-lg-12 text-center">
                <h1 class="mt-5">Security for all in an AI enabled society</h1>
            </div>
        </div>

        <div class="row mt-2 mb-2">
            <section class="col-lg-12">            
                <p>We are organizing a day for bringing together researchers working on the projects funded by the EPSRC call <a href="https://gow.epsrc.ukri.org/NGBOViewPanelROL.aspx?PanelId=1-7RZQS3&RankingListId=1-7RZQW6" target="_blank">"Security for all in an AI enabled society"</a>, which include: <a href="https://research-information.bris.ac.uk/en/projects/chai-cyber-hygiene-in-ai-enabled-domestic-life" target="_blank">CHAI: Cyber Hygiene in AI enabled domestic life</a>, <a href="https://secure-ai-assistants.github.io/" target="_blank">SAIS: Secure AI assistantS</a>, <a href="https://www.macs.hw.ac.uk/aisec/" target="_blank">AISEC: AI Secure and Explainable by Construction</a>, and <a href="https://enncore.github.io/" target="_blank">EnnCore: End-to-End Conceptual Guarding of Neural Architectures</a>.</p> 
                <p>We are particularly interested in discussing recent achievements and future initiatives to address the fundamental security problem of guaranteeing safety, transparency, and robustness in neural-based architectures. We will have invited talks, poster presentations, and a roundtable discussion regarding SafeAI.</p>
                <p>Participants will leave having a better understanding of enabling system designers to specify essential conceptual/behavioral properties of neural-based systems, verify them, and thus safeguard the system against unpredictable behavior and attacks. Therefore, we will foster the dialogue between contemporary explainable neural models and full-stack neural software verification.</p> 
                <p>The event will occur on July 4th in the Department of Computer Science at the University of Manchester, located in the Kilburn Building, Oxford Rd, Manchester M13 9PL. The talks will occur in LT1.4, the poster session in Atlas 1, and the lunch and drinks in the Mercury room. These venues are all located on the first floor of the Kilburn building.</p>
                <p>Participants can find information about parking their cars at <a href="https://www.estates.manchester.ac.uk/services/operationalservices/carparking/">Car Parking at the University</a>. Car Park B: Aquatics Car Park, Manchester M13 9SS, is closest to the Kilburn building. </p>
            </section>          
        </div>

        <div class="row mt-2 mb-2">
        	<section class="col-lg-12">
        		<h1>Program</h1>
                <table>
                    <thead>
                    <tr>
                        <th>Timing</th>
                        <th>Talk</th>    
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>12:40 - 13:20</td>
                        <td>Arrival, sandwiches and coffee (Mercury room)</td>
                    </tr>
                    <tr>
                        <td>13:20 - 13:30</td>
                        <td>Welcome and introduction by <a href="https://ssvlab.github.io/lucasccordeiro/" target="_blank">Lucas Cordeiro</a> (LT1.4)</td>
                    </tr>
                    <tr>
                        <td>13:30 - 14:30</td>
                        <td>
                            <a tabindex="0" style="cursor:pointer;"
                                data-bs-toggle="popover" data-bs-trigger="focus"
                                data-bs-custom-class="custom-popover" data-bs-html="true"
                                data-bs-title="Evaluating Privacy in Machine Learning - Andrew Paverd"
                                data-bs-content="<b>Abstract:</b>
                                <p>
                                    The use of domain-specific private data can bring significant value to ML models, but also requires us to ensure that this data is adequately protected, even from users of the model. Privacy-preserving machine learning has been the focus of a significant body of research, with several mature tools and techniques now available. However, in real-world deployments, several practical questions may still arise: What specific threats are we mitigating? What level of privacy is sufficient? How do we know we have achieved the desired privacy level? In this talk, I will discuss recent work on the theme of evaluating privacy in ML, ranging from the use of 'Privacy Games' to formalize and reason about specific risks, through to techniques for empirically estimating the level of privacy achieved. 
                                </p>
                                <b>About the speaker:</b>
                                <p>
                                    <a href='https://ajpaverd.org/' target='_blank'>Andrew Paverd</a> is a Principal Research Manager in the Microsoft Security Response Center (MSRC), where he leads the strategic research initiative on AI Security & Privacy. In collaboration with researchers across Microsoft, he has been working on tools and techniques to measure and mitigate privacy risks in machine learning. His research interests also include web and systems security. Prior to joining Microsoft, he was a Fulbright Cyber Security Scholar at the University of California, Irvine, and a Research Fellow in the Secure Systems Group at Aalto University. He received his DPhil from the University of Oxford in 2016.
                                </p>
                                ">
                                Evaluating Privacy in Machine Learning (by Andrew Paverd) (LT1.4)
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td>14:30 - 14:45</td>                        
                        <td>Coffee break (Mercury room)</td>
                    </tr>
                    <tr>
                        <td>14:45 - 15:15</td>
                        <td>
                            <a tabindex="0" style="cursor:pointer;"
                                data-bs-toggle="popover" data-bs-trigger="focus"
                                data-bs-custom-class="custom-popover" data-bs-html="true"
                                data-bs-title="A Tale of Two Oracles: Defining and Verifying when AI Systems are Safe - Edoardo Manino"
                                data-bs-content="<b>Abstract:</b>
                                <p>
                                    While the need for safe AI systems is clear, establishing what it means for an AI system to be safe is more difficult. First, how do we encode high-level safety requirements in strict mathematical terms? Second, how do we check that these requirements are met? Offering a (partial) answer to these questions will take us through a journey across several fields: natural language processing, metamorphic software testing, quantised neural networks, equivalence verification and privacy-preserving machine learning.
                                </p>
                                <b>About the speaker:</b>
                                <p>
                                    Dr. Edoardo Manino is a Research Associate in the Department of Computer Science at the University of Manchester. He is part of the Systems and Software Security group and focuses on automated verification of neural networks. His background is in Bayesian machine learning, a topic he got awarded a PhD from the University of Southampton in 2020. Throughout his research career, he published on a wide number of topics, including algorithmic game theory, multi-agent reinforcement learning, network science, crowdsourcing, analog computing and automated software testing.
                                </p>
                                ">
                                A Tale of Two Oracles: Defining and Verifying when AI Systems are Safe (by Edoardo Manino) (LT1.4)
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td>15:40 - 15:55</td>
                        <td>Coffee break (Mercury room)</td>
                    </tr>
                    <tr>
                        <td>15:15 - 15:45</td>
                        <td>
                            <a tabindex="0" style="cursor:pointer;"
                                data-bs-toggle="popover" data-bs-trigger="focus"
                                data-bs-custom-class="custom-popover" data-bs-html="true"
                                data-bs-title="One Picture Paints a Thousand Words: Using Abstract Interpretation for NLP Verification - Marco Casadio"
                                data-bs-content="<b>Abstract:</b><br/> 
                                Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. In this paper, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library that links to the neural network verifier ERAN. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems within this community.">
                                One Picture Paints a Thousand Words: Using Abstract Interpretation for NLP Verification (by Marco Casadio) (LT1.4)
                            </a>
                        </td>
                    </tr>                    
                    <tr>
                        <td>15:45 - 16:00</td>
                        <td>Coffee break (Mercury room)</td>
                    </tr>
                    <tr>
                        <td>16:00 - 16:30</td>
                        <td>
                            <a tabindex="0"  style="cursor:pointer;"
                                data-bs-toggle="popover" data-bs-trigger="focus"
                                data-bs-custom-class="custom-popover" data-bs-html="true"
                                data-bs-title="Efficiently Training Neural Networks for Verifiability - Alessandro De Palma"
                                data-bs-content="<b>Abstract:</b>
                                <p>
                                    Owing to the non-convexity of the associated optimisation problem, neural network verification algorithms do not scale to the network architectures commonly employed in vision or language applications. It is therefore common to replace the problem with an approximation (incomplete methods), providing a formal guarantee on a subset of the target verification properties. While incomplete methods may be relatively inexpensive, they hardly provide any positive answer on standard-trained networks. The accuracy of incomplete verifiers is greatly increased if networks are explicitly trained for verifiability, but these improvements come at a large computational cost, and at the expense of standard performance. In this talk, we present relatively efficient and conceptually simple methods to train networks for verifiability, and outline their potential applications to models employed on language tasks.
                                </p>
                                <b>About the speaker:</b>
                                <p>
                                    Alessandro De Palma is a postdoctoral researcher within the Verification of Autonomous Systems group at Imperial College London, where he works on neural network verification and robustness. He recently completed a PhD at the University of Oxford on the same subject, during which he was awarded an IBM PhD fellowship. His research interests more generally lie at the intersection of optimisation and deep learning.
                                </p>
                                ">
                                Efficiently Training Neural Networks for Verifiability (by Alessandro De Palma) (LT1.4)
                            </a>
                        </td>
                    </tr>
	            <tr>
                        <td>16:30 - 17:00</td>
                        <td>
                            <a tabindex="0"  style="cursor:pointer;"
                                data-bs-toggle="popover" data-bs-trigger="focus"
                                data-bs-custom-class="custom-popover" data-bs-html="true"
                                data-bs-title="Cyber Hygiene in AI-enabled domestic life - George Loukas"
                                data-bs-content="<b>Abstract:</b>
                                <p>
                                    When a user faces a security threat such as receiving a phishing email or visiting a watering hole website, there are often visual and behavioural cues that can raise their suspicion, and there are known cyber hygiene measures they can follow. This is not the case for AI enabled Internet of Things devices such as those found in a modern smart home especially where their AI is the target of an attack. In this talk, we will discuss whether and how users can be involved in the process of protecting themselves, not only in prevention but also in detection, diagnosis and response to an AI security breach.
                                </p>
                                <b>About the speaker:</b>
                                <p>
                                    George Loukas is a Professor of Cyber Security and Head of the new Centre for Sustainable Cyber Security at the University of Greenwich, with focus in cyber-physical security as well as the concept of the human as a security sensor. He has coordinated several national and international research projects in cyber security and is on the editorial board of IEEE TIFS and Elsevier's SIMPAT. His 2015 book defining cyber-physical attacks has been adopted in the curricula of universities internationally. He has a PhD in network security from Imperial College.
                                </p>
                                ">
                                Cyber Hygiene in AI-enabled domestic life (by George Loukas) (LT1.4)
                            </a>
                        </td>
                    </tr>
	            <tr>
                        <td>17:00 - 18:00</td>
                        <td>Drinks reception (Mercury room)</td>
                    </tr>
		    <tr>
                        <td>18:00 - 19:00</td>
                        <td>Free time</td>
                    </tr>
	            <tr>
                        <td>19:00 - 21:00</td>
                        <td>Dinner</td>
                    </tr>
                    </tbody>
                </table>
        	</section>        	
        </div>

        <div class="row mt-4 mb-2">
        	<section class="col-lg-12">
        		<h1>Register</h1>
                <p>This event is by invitation only and is intended to establish new partnerships/collaborations to lead the discussion concerning the challenges and opportunities and tackle our main obstacles to achieving explainable and fully verifiable learning-based systems.</p>
                <p>If you would like to attend, please contact <a href="mailto:lucas.cordeiro@manchester.ac.uk">Lucas Cordeiro</a> and <a href="mailto:Mustafa.Mustafa@manchester.ac.uk">Mustafa Mustafa</a>.</p>
        	</section>
        </div>
    </div>

    <div class="container-fluid">
    	<div class="row">    		
    		<footer id="footer" class="col-lg-12 mt-5 mb-1 text-center" role="contentinfo">
				<a href="https://enncore.github.io/" target="_blank">EnnCore - University of Manchester</a>
				  <div class="social">
					    <a target="_blank" href="https://twitter.com/?????">
					     <svg role="img" title="twitter" width="30" height="30" viewBox="0 0 1792 1792">
					        <path d="M1408 610q-56 25-121 34 68-40 93-117-65 38-134 51-61-66-153-66-87 0-148.5 61.5T883 722q0 29 5 48-129-7-242-65T454 550q-29 50-29 106 0 114 91 175-47-1-100-26v2q0 75 50 133.5t123 72.5q-29 8-51 8-13 0-39-4 21 63 74.5 104t121.5 42q-116 90-261 90-26 0-50-3 148 94 322 94 112 0 210-35.5t168-95 120.5-137 75-162T1304 746q0-18-1-27 63-45 105-109zm256-194v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"></path>
					      </svg>
					   </a>
					   <a target="_blank" href="https://www.linkedin.com/groups/?????">
					      <svg role="img" title="linkedin" width="30" height="30" viewBox="0 0 1792 1792">
					        <path d="M365 1414h231V720H365v694zm246-908q-1-52-36-86t-93-34-94.5 34-36.5 86q0 51 35.5 85.5T479 626h1q59 0 95-34.5t36-85.5zm585 908h231v-398q0-154-73-233t-193-79q-136 0-209 117h2V720H723q3 66 0 694h231v-388q0-38 7-56 15-35 45-59.5t74-24.5q116 0 116 157v371zm468-998v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"></path>
					      </svg>
					    </a>
					    <a target="_blank" href="https://www.youtube.com/channel/?????">
					      <svg role="img" title="youtube" width="30" height="30" viewBox="0 0 1792 1792">
					        <path d="M1047 1303v-157q0-50-29-50-17 0-33 16v224q16 16 33 16 29 0 29-49zm184-122h66v-34q0-51-33-51t-33 51v34zM660 915v70h-80v423h-74V985h-78v-70h232zm201 126v367h-67v-40q-39 45-76 45-33 0-42-28-6-16-6-54v-290h66v270q0 24 1 26 1 15 15 15 20 0 42-31v-280h67zm252 111v146q0 52-7 73-12 42-53 42-35 0-68-41v36h-67V915h67v161q32-40 68-40 41 0 53 42 7 21 7 74zm251 129v9q0 29-2 43-3 22-15 40-27 40-80 40-52 0-81-38-21-27-21-86v-129q0-59 20-86 29-38 80-38t78 38q21 28 21 86v76h-133v65q0 51 34 51 24 0 30-26 0-1 .5-7t.5-16.5V1281h68zM913 457v156q0 51-32 51t-32-51V457q0-52 32-52t32 52zm533 713q0-177-19-260-10-44-43-73.5t-76-34.5q-136-15-412-15-275 0-411 15-44 5-76.5 34.5T366 910q-20 87-20 260 0 176 20 260 10 43 42.5 73t75.5 35q137 15 412 15t412-15q43-5 75.5-35t42.5-73q20-84 20-260zM691 519l90-296h-75l-51 195-53-195h-78l24 69 23 69q35 103 46 158v201h74V519zm289 81V470q0-58-21-87-29-38-78-38-51 0-78 38-21 29-21 87v130q0 58 21 87 27 38 78 38 49 0 78-38 21-27 21-87zm181 120h67V350h-67v283q-22 31-42 31-15 0-16-16-1-2-1-26V350h-67v293q0 37 6 55 11 27 43 27 36 0 77-45v40zm503-304v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"></path>
					      </svg>
					    </a>
				  </div>				  
			</footer>
    	</div>
    </div>


    <script>
        const popoverTriggerList = document.querySelectorAll('[data-bs-toggle="popover"]')
        const popoverList = [...popoverTriggerList].map(popoverTriggerEl => new bootstrap.Popover(popoverTriggerEl))
    </script>
</body>

</html>
